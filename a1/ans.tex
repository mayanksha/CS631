%\documentclass[12pt, a3paper]{extarticle}
\documentclass[12pt, a4paper]{article}
\usepackage{graphicx, rotating, geometry}
\geometry{left=2.0cm, top=1.5cm, right=2.0cm, bottom=2.0cm, footskip=.5cm}
\graphicspath{{./}}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\newcommand{\ti}{\textit}
\newcommand{\ul}{\underline}
\title{Cybersecurity Assignment 1}
\author{Mayank Sharma, 160392}
\date{August, 12, 2019}

\begin{document}
\renewcommand{\v}{\textbf}
\maketitle
\section{}

\subsection{}
Since, the means of communication between the sensor is a computer network, an attacker having access to the network can send fake/corrupted readings (by impersonating \textbf{water level sensor}) to the PLC based controller (assuming that there isn't any kind of message authenctication protocol between the sensors and PLC Controller). \\ \\
In this scenario, attacker will send readings that the water level is below the threshold, which in turn will prompt PLC Controller to open the valves, thereby causing a water overflow.

\subsection{}
Similar to the above case, if the there isn't any kind of message integrity check between the \textbf{temperature sensor} and PLC Controller, an attacker can send readings which show low temperature to PLC Controller. The controller will try to increase the temperature (above any normal temperature), which may be catastrophic.

\subsection{}
Some of the preventative measures we can employ in such a plant:
\begin{itemize}
    \item The network must be secured from any outsider access or even for insiders, there must be different access privileges as to who can access the network directly. All the accesses to such a critical network should be monitored in real-time.
    \item The message integrity between the sensors and the controllers must be maintained by using something like HMAC.
    \item If possible, multiple sensors should be used and their individual readings should be looked at by the controllers. (Redundancy)
    \item An anomaly detection system may as well be deployed to detect any abnormal activity in the system.
\end{itemize}
\newpage
\section{}

From the defintion of inner product, we have
\begin{align*}
    \langle \v{x}, \v{y} \rangle &= \v{x}^{T} \v{y}
\end{align*}
So, for $\langle \v{A}\v{x}, \v{y} \rangle$
\begin{align*}
    \implies \langle \v{A}\v{x}, \v{y} \rangle &= (\v{A}\v{x})^{T} \v{y} \\
    &= ( \v{x}^{T} \v{A}^{T}) \v{y} \\
    &=  \v{x}^{T} (\v{A}^{T} \v{y}) \\
    &=  \langle \v{x}, \v{A}^{T} \v{y} \rangle \\
\end{align*}
Hence, Proved.

\newpage
\section{}
Firstly, $det (\v{A}) \neq 0$. Hence, the matrix $\v{A}$ is invertible and we can proceed forward with its eigen-decomposition. For eigen values, we do $det(\v{A} - \lambda \v{I}) = 0$.
\begin{align*}
    det(\v{A} - \lambda \v{I}) &= 0 \\
    -x^3+14 x^2+24 x+3 &= 0
\end{align*}
The roots of the above equation are
\begin{align*}
    \lambda_1 &= 15.5553 \\
    \lambda_2 &= -1.41941 \\
    \lambda_3 &= -0.13587
\end{align*}
The corresponding eigenvectors (after normalization) are
\begin{align*}
    \v{v}_1 = \left[ {\begin{array}{c}
        0.29983 \\
        0.68205 \\
        1
    \end{array} } \right],
    \v{v}_2 = \left[ {\begin{array}{c}
        -0.963758 \\
        -0.334138 \\
        1
    \end{array} } \right],
    \v{v}_3 = \left[ {\begin{array}{c}
        1.57302 \\
        -2.39338 \\
        1
    \end{array} } \right]
\end{align*}
Now, the matrix $\v{A}$ can be written as $\v{A} = \v{P} \v{D} \v{P}^{-1}$ where $\v{P}$ is a square matrix whose $i^{th}$ column is the $i^{th}$ eigen vector of matrix $\v{A}$ and $\v{D}$ is the diagonal matrix with its diagonal elements as the eigenvalues of $\v{A}$.

\begin{align*}
    \v{P} = \left[ {\begin{array}{ccc}
        \v{v}_1 & \v{v}_2 & \v{v}_3
    \end{array} } \right] \\
    \v{D} = diag(\left[ {\begin{array}{ccc}
        \lambda_1 ,& \lambda_2 ,& \lambda_3
    \end{array} } \right])
\end{align*}
So, we get
\begin{align*}
    \v{P} &= \left[ {\begin{array}{ccc}
        0.29983 & -0.963758 & 1.57302 \\
        0.682059 & -0.334138 & -2.39338 \\
        1 & 1 & 1 \\
    \end{array} } \right] \\
    \v{P}^{-1} &= \left[ {\begin{array}{ccc}
        0.397545 & 0.489735 & 0.546776 \\
        -0.593726 & -0.245794 & 0.345663 \\
        0.196181 & -0.243941 & 0.107561 \\
    \end{array} } \right] \\
    \v{D} &= \left[ {\begin{array}{ccc}
        15.5553 & 0 & 0 \\
        0 & -1.41941 & 0 \\
        0 & 0 & -0.135874 \\
    \end{array} } \right] \\
\end{align*}
To verify, we multiply the three matrices again, and we get
\begin{align*}
\v{P} \v{D} \v{P}^{-1}
    &= \left[ {\begin{array}{ccc}
        1 &  2 &  3 \\
        4 &  4.99999 &  5.99999 \\
        7.00002 &  8 &  8.00001 \\
    \end{array} } \right] \approx \v{A}\\
\end{align*}

\newpage



\section{}
We know that the L-2 Norm of a vector $\v{x}$ is given by,
\begin{align*}
    \Vert \v{x}_2 \Vert^2_2 = \v{x}^T \v{x}
\end{align*}

Since, $\v{Q}$ is an orthogonal matrix, we have
\begin{align*}
    \v{Q}^T \v{Q} = \v{I} =  \v{Q} \v{Q}^T
\end{align*}
Now, left multiplying $\v{x}$ with $\v{Q}$ and gives us $\v{Q} \v{x}$ which is an $n \times 1$ vector. Then taking its L-2 Norm, we get
\begin{align*}
    \Vert \v{Q} \v{x} \Vert^2_2  &= (\v{Q} \v{x})^T (\v{Q} \v{x}) \\
    &= \v{x}^T \v{Q}^T \v{Q} \v{x} \\
    &= \v{x}^T \v{I} \v{x} \\
    &= \v{x}^T \v{x} \\
    &= \Vert \v{x} \Vert^2_2 \\
\end{align*}
Hence, Proved.

\newpage

\section{}

Given that $\v{S}$ is a symmetric matrix (i.e. $\v{S} = \v{S}^T$) with its eigenvectors ($\v{v}_1,\v{v}_2, ..., \v{v}_n$) and the corresponding eigen values ($\lambda_1,\lambda_2, ..., \lambda_n$). Now, by the property of symmetric matrices, these eigenvectors are orthogonal. \\ \\
To Prove:
\begin{align*}
    \v{S} &= \sum_{i=1}^{n} \lambda_{i} \v{v}_i \v{v}^{T}_i\\
\end{align*}
Now, we know that we can decompose $\v{S}$ as $\v{S} = \v{P} \v{D} \v{P}^{-1}$, where $\v{P}$ is a matrix whose columns are the eigenvectors of $\v{S}$ and $\v{D}$ is a diagonal matrix whose diagonal values are eigenvalues of $\v{S}$.
\begin{align*}
    \v{P} = \left[ {\begin{array}{ccc}
        \v{v}_1 & \v{v}_2 & \v{v}_3
    \end{array} } \right] \\
    \v{D} = diag( {\begin{array}{ccc}
        \lambda_1 ,& \lambda_2 ,& \lambda_3
    \end{array} } )
\end{align*}
Since, $\v{P}$ is an orthogonal matrix because all its column vectors are orthogonal, so we have $\v{P}^{-1} = \v{P}^T$.
\begin{align*}
\v{S} = \v{P} \v{D} \v{P}^{-1} = \v{P} \v{D} \v{P}^{T}
\end{align*}
\begin{align*}
\implies \v{S} =
    \left[ {\begin{array}{cccc}
        \v{v}_1,& \v{v}_2,& \ldots,& \v{v}_n
    \end{array} } \right]
    \left[ {\begin{array}{cccc}
        \lambda_1 ,& 0 ,& \ldots ,& 0 \\
        \ldots ,& \lambda_2 ,& \ldots ,& 0 \\
        \ldots ,& \ldots ,& \ldots ,& \ldots \\
        0 ,& 0 ,& \ldots ,& \lambda_n
    \end{array} } \right]
    \left[ {\begin{array}{c}
        \v{v}_1^T \\
        \v{v}_2^T \\
        \ldots \\
        \v{v}_n^T
    \end{array} } \right]
\end{align*}
Once we multiply the above three matrices, we get
\begin{align*}
    \v{S} &= \sum_{i=1}^{n} \lambda_{i} \v{v}_i \v{v}^{T}_i\\
\end{align*}
Hence Proved.
% \textit{Note: On Canvas, sir has mentioned in one of the discussions that we need to asssume that the vectors are orthonormal. } \\

% Given that $\v{S}$ is a symmetric matrix (i.e. $\v{S} = \v{S}^T$) with its eigenvectors ($\v{v}_1,\v{v}_2, ..., \v{v}_n$) and the corresponding eigen values ($\lambda_1,\lambda_2, ..., \lambda_n$). \\
% Consider matrix $\v{Q}$ below,
% \begin{align*}
    % \v{Q} &= \v{S} - \sum_{i=1}^{n} \lambda_{i} \v{v}_i \v{v}^{T}_i\\
% \end{align*}

% Right multiplying by $\v{v}_j$ such that $j \in \{1, 2, ..., n\}$,
% \begin{align*}
    % \v{Q} \v{v}_j &= \v{S} \v{v}_j - \sum_{i=1}^{n} \lambda_{i} \v{v}_i \v{v}^{T}_i \v{v}_j \\
    % &= \v{S} \v{v}_j - \sum_{i=1}^{n} \lambda_{i} \v{v}_i (\v{v}^{T}_i \v{v}_j) \\
    % &= \v{S} \v{v}_j - \sum_{i=1}^{n} \lambda_{i} \v{v}_i \delta_{i, j} \qquad \text{where $\delta_{i, j}$ is the Kronecker Delta, since $\v{v}_i$ and $\v{v}_j$ are orthogonal} \\
% \end{align*}

% So, for the summation term, we get
% \begin{align*}
    % \v{Q} \v{v}_j &= \v{S} \v{v}_j - \lambda_{j} \v{v}_j  \\
% \end{align*}
% From the defintion of a eigenvectors, $\v{S} \v{v}_j = \lambda_{j} \v{v}_j$, so we get $\v{Q} = \v{0}$ or
% \[
    % \v{S} = \sum_{i=1}^{n} \lambda_{i} \v{v}_i \v{v}^{T}_i
% \]

\newpage

\section{}
Given $\v{A}_{m \times n}$ matrix, the kernel of $\v{A}$ is defined as
\begin{align*}
    \text{ker} (\v{A}) &= \{ \v{x} \mid \v{A} \v{x} = 0 \}
\end{align*}
and the row space of $\v{A}$ is defined as taking $\v{A} =
\left[ {\begin{array}{c}
    \v{a}_1^T \\
    \v{a}_2^T \\
 \ldots \\
 \ldots \\
    \v{a}_m^T
 \end{array} } \right]$
\begin{align*}
    \text{row} (\v{A}) &= \{ \v{y} \mid \v{y} = \sum_{i=1}^{m} \v{a}_i c_i,\qquad c_i \in \mathbb{R}\}
\end{align*}
Now, from the defintion of ker $(\v{A})$, we have
\begin{align*}
    &\v{A}_{m \times n}  \hskip0.5em \v{x}_{n \times 1} = 0 \\
    &\implies \v{a}_i^T \v{x} = 0
\end{align*}
To check for orthogonality, we arbitrarily take any vector $\v{y}$ from $\text{row} (\v{A})$ and similarly, $\v{x}$ from $\text{ker} (\v{A})$ and check using their inner product. If the inner product is 0, then the two spaces are orthogonal to each other.
\begin{align*}
    \langle \v{y}, \v{x} \rangle &= \v{y}^T \v{x} \\
    &= \left (\sum_{i=1}^m c_i\v{a}_i \right)^T  \v{x} \\
    &= \left (\sum_{i=1}^m c_i\v{a}_i \right)^T  \v{x} \\
    &=  \left (\sum_{i=1}^m c_i \v{a}_i^T\right)  \v{x} \\
    &= \sum_{i=1}^m c_i (\v{a}_i^T  \v{x}) \\
    &= 0
\end{align*}
Hence, the kernel space of $\v{A}$ and row space are orthogonal to each other.

\newpage
\section{}

The SVD is defined as follows: \\ Let $\v{A}$ be an $(m \times n)$ matrix with $m \geq n$.
Then there exist orthogonal matrices $\v{U}_{m \times m}$ and $\v{V}_{n \times n}$ and a diagonal matrix $\Sigma_{(m \times n)} = \text{diag} (\sigma_1, \ldots , \sigma_n)$ with $\sigma_1 \geq \sigma_2 \geq . . . \geq \sigma_n \geq 0$, such that
\begin{align*}
    \v{A} = \v{U} \Sigma \v{V}^T
\end{align*}
The values $\sigma_i$ are called the singular values of $\v{A}$. \\ \\
We know, If $\v{A} = \v{U}\Sigma\v{V}^T$, then the column vectors of $\v{V}$ are the eigenvectors of the matrix $\v{A}^T\v{A}$ and the column vectors of matrix $\v{U}$ are the eigenvectors of the matrix $\v{A}\v{A}^T$.\\ \\
Now, the matrices $\v{A}^T\v{A}$ and $\v{A}\v{A}^T$ are symmetrical (because $(\v{A}^T\v{A})^T = \v{A}^T\v{A}$, and $(\v{A}\v{A}^T)^T = \v{A}\v{A}^T$).
\begin{align*}
    \v{A}^T\v{A} = (\v{U}\Sigma\v{V})^T(\v{U}\Sigma\v{V}) = \v{V}\v{D}\v{V}^T, \\
    \text{where $\v{D} = \Sigma^T \Sigma = diag$ $(\sigma_1^2, \ldots , \sigma_n^2)$} 
\end{align*}
Also,
\begin{align*}
    \v{A}\v{A}^T = (\v{U}\Sigma\v{V})(\v{U}\Sigma\v{V})^T = \v{U}^T\Sigma\Sigma^T\v{U}^T, \\
    \text{where $\Sigma\Sigma^T = diag$ $(\sigma_1^2, \ldots , \sigma_n^2, 0, \ldots, 0)$} 
\end{align*}
We further re-instate the fact that $\v{U}$ and $\v{V}$ are orthogonal by the property that the eigenvectors of a symmetrical matrices (here, $\v{A}^T\v{A}$ and $\v{A}\v{A}^T$) are orthogonal. \\ \\

Now, Calculating the SVD consists of finding the eigenvalues and eigenvectors of $\v{A}^T\v{A}$ and $\v{A}\v{A}^T$ .
The eigenvectors of $\v{A}^T\v{A}$ make up the columns of $\v{V}$, and the eigenvectors of $\v{A}\v{A}^T$  make up the columns of $\v{U}$. \\ \\
Suppose the eigenvalues of $\v{A}^T\v{A}$ are $(\sigma_1, \ldots , \sigma_n)$ such that $(\sigma_1 \geq \sigma_2 \eq \ldots \geq \sigma_n \geq 0)$ and corresponding eigenvectors are $(\v{x}_1, \ldots , \v{x}_n)$ , hence the singular values in $\Sigma$ are $(\sqrt{\sigma_1}, \ldots , \sqrt{\sigma_n})$. Let $\lambda_i = \sqrt{\sigma_i}$, so that $\lambda_i$ is a singular value of $\Sigma$. Also, let $\v{u}_i = \frac{\v{A}\v{x}_i}{\lambda_i}$. The matrix $\v{U}$ has the colums $\v{u}_i$ and the matrix $\v{V}$ has the colums $\v{x}_i$. \\ \\
Lets take a look at the multiplication in $\v{A} = \v{U}\Sigma\v{V}^T$ step by step.
Firstly, we have $\v{U}$ which we multiply by the diagonal matrix $\Sigma_{m \times n}$ (with diagonal values set to $\sigma_i$, and possibly padded with zeros if we run out of eigenvalues). Hence, we get a matrix with colums (this is basically scaling the colums of $\v{U}$ with factors equal to singular values in $\Sigma$) \\
% \begin{align*}
% \v{u}_i \sigma_i = \frac{\v{A} \v{x}_i}{\sigma_i}\sigma_i  = \v{A} \v{x}_i.
% \end{align*}

Then, we multiple $\v{U}\Sigma$ with $\v{V}^T$, and the product contains terms like $\v{A} \v{x}_i \v{x}^T_j$, but since $\v{x}_i$ and $\v{x}_j$ are the eigenvectors of a symmetric matrix, $\v{x}_i$ and $\v{x}_j$ form an orthogonal basis.
The significance of multiplcation of $V^T$ term is that it basically transforms (rotates) a matrix in the n-dimensional space.

\textbf{Applications of SVD:} SVD is used for dimensionality reduction, meaning that we can remove those components of a matrix which have very small singular values. This has the benefit that it reduces computation costs. This is highly used in lossy image compression in the cases where we're fine if we lose some information at the benefit of being able to store the image at a smaller size.

\end{document}
